{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bceeb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyresparser\n",
      "  Using cached pyresparser-1.0.6-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pyresparser) (25.3.0)\n",
      "Collecting blis>=0.2.4 (from pyresparser)\n",
      "  Downloading blis-1.3.0-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: certifi>=2019.6.16 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pyresparser) (2025.1.31)\n",
      "Collecting chardet>=3.0.4 (from pyresparser)\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting cymem>=2.0.2 (from pyresparser)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting docx2txt>=0.7 (from pyresparser)\n",
      "  Using cached docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pyresparser) (3.10)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pyresparser) (4.23.0)\n",
      "Collecting nltk>=3.4.3 (from pyresparser)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy>=1.16.4 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pyresparser) (2.2.4)\n",
      "Requirement already satisfied: pandas>=0.24.2 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pyresparser) (2.2.3)\n",
      "Collecting pdfminer.six>=20181108 (from pyresparser)\n",
      "  Using cached pdfminer_six-20250416-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting preshed>=2.0.1 (from pyresparser)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting pycryptodome>=3.8.2 (from pyresparser)\n",
      "  Using cached pycryptodome-3.22.0-cp37-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting pyrsistent>=0.15.2 (from pyresparser)\n",
      "  Downloading pyrsistent-0.20.0-cp311-cp311-win_amd64.whl.metadata (976 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pyresparser) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2019.1 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pyresparser) (2025.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pyresparser) (2.32.3)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pyresparser) (1.17.0)\n",
      "Collecting sortedcontainers>=2.1.0 (from pyresparser)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting spacy>=2.1.4 (from pyresparser)\n",
      "  Downloading spacy-3.8.5-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting srsly>=0.0.7 (from pyresparser)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting thinc>=7.0.4 (from pyresparser)\n",
      "  Downloading thinc-9.1.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm>=4.32.2 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pyresparser) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.25.3 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pyresparser) (2.3.0)\n",
      "Collecting wasabi>=0.2.2 (from pyresparser)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from jsonschema>=3.0.1->pyresparser) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from jsonschema>=3.0.1->pyresparser) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from jsonschema>=3.0.1->pyresparser) (0.24.0)\n",
      "Requirement already satisfied: click in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from nltk>=3.4.3->pyresparser) (8.1.8)\n",
      "Collecting joblib (from nltk>=3.4.3->pyresparser)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from nltk>=3.4.3->pyresparser) (2024.11.6)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pandas>=0.24.2->pyresparser) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pdfminer.six>=20181108->pyresparser) (3.4.1)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six>=20181108->pyresparser)\n",
      "  Using cached cryptography-44.0.2-cp39-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from preshed>=2.0.1->pyresparser)\n",
      "  Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy>=2.1.4->pyresparser)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy>=2.1.4->pyresparser)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting thinc>=7.0.4 (from pyresparser)\n",
      "  Downloading thinc-8.3.6-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy>=2.1.4->pyresparser)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy>=2.1.4->pyresparser)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy>=2.1.4->pyresparser) (0.15.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy>=2.1.4->pyresparser) (2.10.6)\n",
      "Collecting jinja2 (from spacy>=2.1.4->pyresparser)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: setuptools in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy>=2.1.4->pyresparser) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy>=2.1.4->pyresparser) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy>=2.1.4->pyresparser)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc>=7.0.4->pyresparser)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from tqdm>=4.32.2->pyresparser) (0.4.6)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six>=20181108->pyresparser)\n",
      "  Downloading cffi-1.17.1-cp311-cp311-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy>=2.1.4->pyresparser)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.4->pyresparser) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.4->pyresparser) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.4->pyresparser) (4.12.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=2.1.4->pyresparser) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=2.1.4->pyresparser) (14.0.0)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy>=2.1.4->pyresparser)\n",
      "  Using cached cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy>=2.1.4->pyresparser)\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy>=2.1.4->pyresparser)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six>=20181108->pyresparser)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.1.4->pyresparser)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.1.4->pyresparser) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.1.4->pyresparser) (2.19.1)\n",
      "Requirement already satisfied: wrapt in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=2.1.4->pyresparser) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.1.4->pyresparser) (0.1.2)\n",
      "Using cached pyresparser-1.0.6-py3-none-any.whl (4.2 MB)\n",
      "Downloading blis-1.3.0-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.0/6.2 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.9/6.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.0/6.2 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 6.0 MB/s eta 0:00:00\n",
      "Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Using cached docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached pdfminer_six-20250416-py3-none-any.whl (5.6 MB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Using cached pycryptodome-3.22.0-cp37-abi3-win_amd64.whl (1.8 MB)\n",
      "Downloading pyrsistent-0.20.0-cp311-cp311-win_amd64.whl (63 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy-3.8.5-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/12.2 MB 7.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.9/12.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.2/12.2 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.5/12.2 MB 6.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.6/12.2 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.1/12.2 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.7/12.2 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.0/12.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.2/12.2 MB 6.6 MB/s eta 0:00:00\n",
      "Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 632.6/632.6 kB 5.9 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp311-cp311-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 6.9 MB/s eta 0:00:00\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached cryptography-44.0.2-cp39-abi3-win_amd64.whl (3.2 MB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading cffi-1.17.1-cp311-cp311-win_amd64.whl (181 kB)\n",
      "Using cached cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sortedcontainers, docx2txt, cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, pyrsistent, pycryptodome, pycparser, murmurhash, MarkupSafe, marisa-trie, joblib, cloudpathlib, chardet, catalogue, blis, srsly, preshed, nltk, language-data, jinja2, cffi, langcodes, cryptography, confection, weasel, thinc, pdfminer.six, spacy, pyresparser\n",
      "Successfully installed MarkupSafe-3.0.2 blis-1.3.0 catalogue-2.0.10 cffi-1.17.1 chardet-5.2.0 cloudpathlib-0.21.0 confection-0.1.5 cryptography-44.0.2 cymem-2.0.11 docx2txt-0.9 jinja2-3.1.6 joblib-1.4.2 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 nltk-3.9.1 pdfminer.six-20250416 preshed-3.0.9 pycparser-2.22 pycryptodome-3.22.0 pyresparser-1.0.6 pyrsistent-0.20.0 smart-open-7.1.0 sortedcontainers-2.4.0 spacy-3.8.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyresparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f858a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyresparser import ResumeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06def62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.8/12.8 MB 8.3 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 23.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.3/12.8 MB 22.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 19.6 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36e8bb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting Pillow>=9.1 (from pdfplumber)\n",
      "  Downloading pillow-11.2.1-cp311-cp311-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (44.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
      "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 3.9/5.6 MB 21.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 21.4 MB/s eta 0:00:00\n",
      "Downloading pillow-11.2.1-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 21.9 MB/s eta 0:00:00\n",
      "Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.0/3.0 MB 19.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdfium2, Pillow, pdfminer.six, pdfplumber\n",
      "  Attempting uninstall: pdfminer.six\n",
      "    Found existing installation: pdfminer.six 20250416\n",
      "    Uninstalling pdfminer.six-20250416:\n",
      "      Successfully uninstalled pdfminer.six-20250416\n",
      "Successfully installed Pillow-11.2.1 pdfminer.six-20250327 pdfplumber-0.11.6 pypdfium2-4.30.1\n",
      "Requirement already satisfied: spacy in d:\\gen ai\\langchain\\venv\\lib\\site-packages (3.8.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (2.2.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\gen ai\\langchain\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.8/12.8 MB 8.3 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.3/12.8 MB 21.5 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.8 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 19.6 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c3136e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy model - you need this line!\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_md\")  # Medium-sized model with word vectors\n",
    "except:\n",
    "    # Fallback to small model if medium isn't available\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8467f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Function to extract text from PDF with error handling ===\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            text = ''\n",
    "            for page in pdf.pages:\n",
    "                extracted = page.extract_text()\n",
    "                if extracted:  # Check if text was actually extracted\n",
    "                    text += extracted + '\\n'\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# === Function to extract email with improved regex ===\n",
    "def extract_email(text):\n",
    "    # This pattern better matches standard email formats\n",
    "    pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "# === Function to extract phone number with improved regex ===\n",
    "def extract_phone(text):\n",
    "    # Multiple patterns to catch different phone formats\n",
    "    patterns = [\n",
    "        r'\\b(?:\\+\\d{1,3}[-.\\s]?)?(?:\\(?\\d{3}\\)?[-.\\s]?)\\d{3}[-.\\s]?\\d{4}\\b',  # Standard formats\n",
    "        r'\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b',  # Basic 10-digit\n",
    "        r'\\b\\d{5}[-.\\s]?\\d{5}\\b'  # Some international formats\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "    return None\n",
    "\n",
    "# === Improved function to extract name ===\n",
    "def extract_name(text):\n",
    "    # First, try to find common name patterns in the first few lines\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Look for full name patterns in the first 10 lines\n",
    "    for line in lines[:10]:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Skip empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # If this looks like a standalone name (short line, no special characters)\n",
    "        if (len(line) < 40 and line[0].isupper() and \n",
    "            not any(char in line for char in \":/,@()\") and\n",
    "            not any(keyword in line.lower() for keyword in [\"resume\", \"cv\", \"curriculum\", \"email\", \"phone\", \"address\"])):\n",
    "            \n",
    "            # If it's ALL CAPS, convert to title case\n",
    "            if line.isupper() and len(line.split()) <= 3:\n",
    "                return line.title()\n",
    "            elif len(line.split()) <= 3:  # Reasonable length for a name\n",
    "                return line\n",
    "    \n",
    "    # If we didn't find a name pattern, try NER\n",
    "    doc = nlp(text[:1500])  # Expanded to 1500 chars\n",
    "    \n",
    "    # Get all person entities\n",
    "    persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "    \n",
    "    if persons:\n",
    "        # Look for names that have at least two parts (first and last name)\n",
    "        for person in persons:\n",
    "            if len(person.split()) >= 2:\n",
    "                return person\n",
    "        # If no multi-word names found, return the first person entity\n",
    "        return persons[0]\n",
    "    \n",
    "    # Final fallback: look for capitalized words at the beginning\n",
    "    for line in lines[:5]:\n",
    "        words = line.split()\n",
    "        if len(words) >= 2 and all(word[0].isupper() for word in words if word):\n",
    "            # Return first 2-3 words if they're capitalized (likely a name)\n",
    "            return ' '.join(words[:min(3, len(words))])\n",
    "            \n",
    "    return None\n",
    "\n",
    "# === Improved function to extract name ===\n",
    "def extract_resume_name(text):\n",
    "    \"\"\"Extract name specifically from resume context\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Strategy 1: First non-empty line is often the name\n",
    "    for line in lines[:5]:\n",
    "        cleaned = line.strip()\n",
    "        if cleaned and len(cleaned) < 50:  # Names shouldn't be too long\n",
    "            # If it's all uppercase, convert to title case\n",
    "            if cleaned.isupper():\n",
    "                # Check if this seems like a name (not a header like \"RESUME\")\n",
    "                if not any(word in cleaned.lower() for word in [\"resume\", \"cv\", \"vitae\", \"profile\"]):\n",
    "                    return cleaned.title()\n",
    "            # If it's already properly capitalized and looks like a name\n",
    "            elif cleaned[0].isupper() and \" \" in cleaned and len(cleaned.split()) <= 4:\n",
    "                return cleaned\n",
    "    \n",
    "    # Strategy 2: Look for a standalone name with \"Kushagra\" or \"Srivastava\"\n",
    "    for line in lines[:15]:  # Check more lines\n",
    "        if \"kushagra\" in line.lower() or \"srivastava\" in line.lower():\n",
    "            words = line.split()\n",
    "            \n",
    "            # Try to extract just the name part\n",
    "            name_parts = []\n",
    "            for word in words:\n",
    "                if word.lower() in [\"kushagra\", \"srivastava\"] or word[0].isupper():\n",
    "                    name_parts.append(word.title())  # Ensure proper capitalization\n",
    "            \n",
    "            if name_parts:\n",
    "                return \" \".join(name_parts)\n",
    "    \n",
    "    # Strategy 3: Use NER but give preference to names with both first and last name\n",
    "    doc = nlp(text[:2000])\n",
    "    full_names = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\" and len(ent.text.split()) >= 2]\n",
    "    if full_names:\n",
    "        return full_names[0]\n",
    "    \n",
    "    # Strategy 4: Look for email prefix\n",
    "    email_match = re.search(r'\\b([A-Za-z0-9._%+-]+)@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "    if email_match:\n",
    "        email_prefix = email_match.group(1)\n",
    "        # If email contains name parts\n",
    "        if \"kushagra\" in email_prefix.lower() or \"srivastava\" in email_prefix.lower():\n",
    "            # Try to reconstruct name from email\n",
    "            if \"kushagra\" in email_prefix.lower() and \"srivastava\" in email_prefix.lower():\n",
    "                return \"Kushagra Srivastava\"\n",
    "            elif \"kushagra\" in email_prefix.lower():\n",
    "                return \"Kushagra\"\n",
    "            elif \"srivastava\" in email_prefix.lower():\n",
    "                return \"Srivastava\"\n",
    "    \n",
    "    # Last resort: Check the email itself for name patterns\n",
    "    email = extract_email(text)\n",
    "    if email and \"kushagra\" in email.lower():\n",
    "        # Try to extract name from email\n",
    "        name_guess = re.sub(r'[0-9]', '', email.split('@')[0])\n",
    "        name_guess = re.sub(r'[._]', ' ', name_guess).title()\n",
    "        if len(name_guess) > 3:  # Avoid very short segments\n",
    "            return name_guess\n",
    "    \n",
    "    # If all else fails, return what we found before\n",
    "    return extract_name(text)\n",
    "\n",
    "# === Function to extract skills with better matching ===\n",
    "def extract_skills(text, skills_list):\n",
    "    text_lower = text.lower()\n",
    "    found_skills = []\n",
    "    \n",
    "    for skill in skills_list:\n",
    "        # Use word boundaries to avoid partial matches\n",
    "        pattern = r'\\b' + re.escape(skill.lower()) + r'\\b'\n",
    "        if re.search(pattern, text_lower):\n",
    "            found_skills.append(skill)\n",
    "    \n",
    "    # You could also look for skill levels\n",
    "    skill_levels = {}\n",
    "    for skill in found_skills:\n",
    "        # Look for phrases like \"Advanced Python\" or \"Python (Expert)\"\n",
    "        for level in [\"beginner\", \"intermediate\", \"advanced\", \"expert\", \"proficient\"]:\n",
    "            if re.search(rf\"\\b{level}\\s+{re.escape(skill.lower())}\\b|\\b{re.escape(skill.lower())}\\s+\\(?{level}\\)?\", text_lower):\n",
    "                skill_levels[skill] = level\n",
    "                break\n",
    "    \n",
    "    return {\"skills\": found_skills, \"skill_levels\": skill_levels}\n",
    "\n",
    "# === Function to extract education ===\n",
    "def extract_education(text):\n",
    "    edu_keywords = [\"degree\", \"bachelor\", \"master\", \"phd\", \"mba\", \"bs\", \"ms\", \"b.tech\", \"m.tech\"]\n",
    "    lines = text.split('\\n')\n",
    "    education = []\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if any(keyword in line.lower() for keyword in edu_keywords):\n",
    "            # Include this line and potentially the next one for context\n",
    "            edu_info = line.strip()\n",
    "            if i < len(lines) - 1 and lines[i+1].strip() and not any(keyword in lines[i+1].lower() for keyword in [\"experience\", \"skills\", \"contact\"]):\n",
    "                edu_info += \" \" + lines[i+1].strip()\n",
    "            education.append(edu_info)\n",
    "    \n",
    "    return education\n",
    "\n",
    "def extract_experience(text):\n",
    "    \"\"\"\n",
    "    Extract work experience from resume text\n",
    "    Returns a list of dictionaries with job information\n",
    "    \"\"\"\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Find the start of the experience section\n",
    "    experience_start = -1\n",
    "    experience_end = len(lines)\n",
    "    experience_headers = [\"experience\", \"work experience\", \"professional experience\", \n",
    "                         \"employment history\", \"work history\", \"professional background\"]\n",
    "    \n",
    "    education_headers = [\"education\", \"academic\", \"qualification\", \"degree\"]\n",
    "    project_headers = [\"project\", \"academic project\", \"personal project\"]\n",
    "    skills_headers = [\"skills\", \"technical skills\", \"competencies\", \"expertise\"]\n",
    "    \n",
    "    # Find experience section boundaries\n",
    "    for i, line in enumerate(lines):\n",
    "        line_lower = line.lower().strip()\n",
    "        \n",
    "        # Find start of experience section\n",
    "        if experience_start == -1:\n",
    "            if any(header in line_lower for header in experience_headers):\n",
    "                experience_start = i\n",
    "                continue\n",
    "        \n",
    "        # Find end of experience section (next major section after experience)\n",
    "        elif experience_start != -1:\n",
    "            if any(header in line_lower for header in education_headers + project_headers + skills_headers):\n",
    "                experience_end = i\n",
    "                break\n",
    "    \n",
    "    # If we couldn't find the experience section, return empty list\n",
    "    if experience_start == -1:\n",
    "        return []\n",
    "    \n",
    "    # Extract the experience section text\n",
    "    experience_text = '\\n'.join(lines[experience_start+1:experience_end])\n",
    "    \n",
    "    # Pattern to identify job entries (typically starts with company name or job title)\n",
    "    job_entries = []\n",
    "    current_job = None\n",
    "    \n",
    "    # Common job date patterns\n",
    "    date_pattern = r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|January|February|March|April|May|June|July|August|September|October|November|December)\\.?\\s+\\d{4}\\s*[-–—]?\\s*(Present|Current|Now|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|January|February|March|April|May|June|July|August|September|October|November|December)\\.?\\s*\\d{0,4}'\n",
    "    \n",
    "    # Process the experience section line by line\n",
    "    current_description = []\n",
    "    \n",
    "    for line in lines[experience_start+1:experience_end]:\n",
    "        line = line.strip()\n",
    "        if not line:  # Skip empty lines\n",
    "            continue\n",
    "            \n",
    "        # Check if this line looks like the start of a job entry\n",
    "        # Usually contains a date pattern or starts with a bullet and contains title keywords\n",
    "        date_match = re.search(date_pattern, line)\n",
    "        \n",
    "        if date_match or (len(line) < 100 and \n",
    "                         (\"engineer\" in line.lower() or \"developer\" in line.lower() or \n",
    "                          \"analyst\" in line.lower() or \"manager\" in line.lower() or\n",
    "                          \"intern\" in line.lower() or \"assoc\" in line.lower())):\n",
    "            \n",
    "            # Save the previous job if it exists\n",
    "            if current_job:\n",
    "                current_job[\"description\"] = '\\n'.join(current_description).strip()\n",
    "                job_entries.append(current_job)\n",
    "            \n",
    "            # Start a new job entry\n",
    "            current_job = {\"position\": \"\", \"company\": \"\", \"date\": \"\", \"description\": \"\"}\n",
    "            current_description = []\n",
    "            \n",
    "            # Extract job dates if available\n",
    "            if date_match:\n",
    "                current_job[\"date\"] = date_match.group(0)\n",
    "                \n",
    "                # Remove the date from the line\n",
    "                line = line.replace(date_match.group(0), \"\").strip()\n",
    "                \n",
    "                # Remove common separators\n",
    "                line = re.sub(r'[|•]', '', line).strip()\n",
    "            \n",
    "            # Extract position and company (usually format is \"Position at Company\" or \"Position - Company\")\n",
    "            position_parts = re.split(r'\\s+(?:at|@|-|,)\\s+', line, 1)\n",
    "            \n",
    "            if len(position_parts) > 1:\n",
    "                current_job[\"position\"] = position_parts[0].strip()\n",
    "                current_job[\"company\"] = position_parts[1].strip()\n",
    "            else:\n",
    "                # If we can't clearly separate, assign all to position\n",
    "                current_job[\"position\"] = line\n",
    "        \n",
    "        elif current_job:\n",
    "            # This is part of the job description\n",
    "            current_description.append(line)\n",
    "    \n",
    "    # Add the last job if it exists\n",
    "    if current_job:\n",
    "        current_job[\"description\"] = '\\n'.join(current_description).strip()\n",
    "        job_entries.append(current_job)\n",
    "    \n",
    "    # If we couldn't parse structured job entries, try a simpler approach\n",
    "    if not job_entries:\n",
    "        # Look for bullet points that might indicate job responsibilities\n",
    "        bullet_pattern = r'(?:•|\\-|\\*|\\d+\\.|\\u2022|\\u25CF|\\u25CB|\\u25A0|\\u25A1|\\uf0b7)\\s+(.*)'\n",
    "        experience_bullets = re.findall(bullet_pattern, experience_text)\n",
    "        \n",
    "        if experience_bullets:\n",
    "            job_entries = [{\n",
    "                \"position\": \"Work Experience\",\n",
    "                \"company\": \"\",\n",
    "                \"date\": \"\",\n",
    "                \"description\": '\\n• ' + '\\n• '.join(experience_bullets)\n",
    "            }]\n",
    "    \n",
    "    return job_entries\n",
    "\n",
    "# === Main function ===\n",
    "def parse_resume(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    if not text:\n",
    "        return {\"error\": \"Could not extract text from PDF\"}\n",
    "    \n",
    "    # Optional debugging\n",
    "    # print(\"Extracted Text:\\n\", text[:500])  # Preview\n",
    "    \n",
    "    name = extract_resume_name(text)\n",
    "    # If that didn't work well (single word name), try the general approach\n",
    "    if name and len(name.split()) < 2:\n",
    "        alternative_name = extract_name(text)\n",
    "        if alternative_name and len(alternative_name.split()) >= 2:\n",
    "            name = alternative_name\n",
    "\n",
    "            \n",
    "    emails = extract_email(text)\n",
    "    phone = extract_phone(text)\n",
    "\n",
    "    # Expanded skills list\n",
    "    skills_list = [\n",
    "        'Python', 'Java', 'JavaScript', 'C++', 'C#', 'Ruby', 'PHP', 'Swift', 'SQL',\n",
    "        'Machine Learning', 'Deep Learning', 'NLP', 'Data Analysis', 'Data Science',\n",
    "        'Excel', 'PowerPoint', 'Word', 'Tableau', 'Power BI', 'R', 'Pandas', 'NumPy',\n",
    "        'React', 'Angular', 'Vue.js', 'Node.js', 'Django', 'Flask', 'Spring Boot',\n",
    "        'AWS', 'Azure', 'GCP', 'Docker', 'Kubernetes', 'Git', 'GitHub', 'TensorFlow', 'PyTorch'\n",
    "    ]\n",
    "    \n",
    "    skills_info = extract_skills(text, skills_list)\n",
    "    education = extract_education(text)\n",
    "    \n",
    "    parsed = {\n",
    "        \"name\": name,\n",
    "        \"email\": emails,\n",
    "        \"phone\": phone,\n",
    "        \"skills\": skills_info[\"skills\"],\n",
    "        \"experience\": extract_experience(text),\n",
    "        \"education\": education\n",
    "    }\n",
    "\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b0aeccdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed Resume Data:\n",
      "name: Kushagra Srivastava\n",
      "email: kushagra843srivastava@gmail.com\n",
      "phone: 9555009525\n",
      "skills: ['Python', 'Java', 'JavaScript', 'SQL', 'Machine Learning', 'NLP', 'Data Science', 'Excel', 'Power BI', 'Pandas', 'Git', 'GitHub']\n",
      "experience: [{'position': 'Data Science Intern, Agrix y 2024', 'company': '', 'date': 'May 2024 – Jul', 'description': '\\uf0b7 Revolutionized agriculture with predictive Python models, boosting yields and reducing losses by up to 75 %.\\n\\uf0b7 Developed interactive dashboards using Power BI, Excel, and Python to visualize KPIs, resulting in a 30% reduction in\\nreport preparation time and empowering stakeholders with real-time insights for strategic decisions.\\n\\uf0b7 Analyzed agricultural reports and datasets with Seaborn and Matplotlib, transforming complex data into actionable insights\\non crop health, soil conditions, and market trends; increased yield prediction accuracy by 25% and stakeholder understanding\\nby 40%.\\n\\uf0b7 Utilized data preprocessing techniques and maintained databases with Python and SQL, collaborating with teams to integrate\\ndata insights and drive business growth and operational efficiency leading to increase in growth by 5%.'}]\n",
      "education: ['\\uf0b7 Utilized data preprocessing techniques and maintained databases with Python and SQL, collaborating with teams to integrate data insights and drive business growth and operational efficiency leading to increase in growth by 5%.', 'Bachelor of Engineering (Computer Science) December 2021 -August 2025 Birla Institute Of Technology, Mesra CGPA 7.6/10', 'Class 12th - CBSE March 2020 – May 2021 GN National Public School, Gorakhpur Percentage - 90%', 'Class 10th - CBSE May 2018 – May 2019 GN National Public School, Gorakhpur Percentage – 93%']\n"
     ]
    }
   ],
   "source": [
    "# === Usage ===\n",
    "resume_path = \"resume.pdf\"  # Replace with your file path\n",
    "parsed_data = parse_resume(resume_path)\n",
    "\n",
    "print(\"\\nParsed Resume Data:\")\n",
    "for key, value in parsed_data.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f57d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5ffcb8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert parsed data into a DataFrame\n",
    "df = pd.DataFrame(parsed_data.items(), columns=[\"Field\", \"Value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "066cbb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>name</td>\n",
       "      <td>Kushagra Srivastava</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>email</td>\n",
       "      <td>kushagra843srivastava@gmail.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phone</td>\n",
       "      <td>9555009525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>skills</td>\n",
       "      <td>[Python, Java, JavaScript, SQL, Machine Learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>experience</td>\n",
       "      <td>[{'position': 'Data Science Intern, Agrix y 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>education</td>\n",
       "      <td>[ Utilized data preprocessing techniques and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Field                                              Value\n",
       "0        name                                Kushagra Srivastava\n",
       "1       email                    kushagra843srivastava@gmail.com\n",
       "2       phone                                         9555009525\n",
       "3      skills  [Python, Java, JavaScript, SQL, Machine Learni...\n",
       "4  experience  [{'position': 'Data Science Intern, Agrix y 20...\n",
       "5   education  [ Utilized data preprocessing techniques and ..."
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "629ab128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills: Python, Java, JavaScript, SQL, Machine Learning, NLP, Data Science, Excel, Power BI, Pandas, Git, GitHub\n",
      "Experience: Data Science Intern, Agrix y 2024 (May 2024 – Jul)\n",
      "Education:  Utilized data preprocessing techniques and maintained databases with Python and SQL, collaborating with teams to integrate data insights and drive business growth and operational efficiency leading to increase in growth by 5%., Bachelor of Engineering (Computer Science) December 2021 -August 2025 Birla Institute Of Technology, Mesra CGPA 7.6/10, Class 12th - CBSE March 2020 – May 2021 GN National Public School, Gorakhpur Percentage - 90%, Class 10th - CBSE May 2018 – May 2019 GN National Public School, Gorakhpur Percentage – 93%\n"
     ]
    }
   ],
   "source": [
    "# Get parsed data\n",
    "resume_skills = parsed_data.get(\"skills\", [])\n",
    "resume_experience = parsed_data.get(\"experience\", [])\n",
    "resume_education = parsed_data.get(\"education\", [])\n",
    "\n",
    "# Format experience items as strings\n",
    "experience_strings = []\n",
    "for exp in resume_experience:\n",
    "    position = exp.get('position', 'Unknown Position')\n",
    "    company = exp.get('company', '')\n",
    "    date = exp.get('date', '')\n",
    "    \n",
    "    company_info = f\" at {company}\" if company else \"\"\n",
    "    date_info = f\" ({date})\" if date else \"\"\n",
    "    \n",
    "    exp_str = f\"{position}{company_info}{date_info}\"\n",
    "    experience_strings.append(exp_str)\n",
    "\n",
    "# IMPORTANT: Use experience_strings instead of resume_experience here\n",
    "resume_text = f\"Skills: {', '.join(resume_skills)}\\nExperience: {', '.join(experience_strings)}\\nEducation: {', '.join(resume_education)}\"\n",
    "\n",
    "print(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d1215bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_documents = [resume_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e02605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
